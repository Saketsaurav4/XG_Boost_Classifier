{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Installing the Libraries\n"
      ],
      "metadata": {
        "id": "xr8oDXBWGhrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn8mvmo8bRXF",
        "outputId": "c2199f48-1ecb-45e0-a1e5-239a2c5f6aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=67bff200352d12431485dbdb5d1346035b6533327414d6ebf07caefe31d49e8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark xgboost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This command installs two libraries:**\n",
        "\n",
        "**pyspark:** This is the Python API for Apache Spark, a powerful open-source distributed computing framework used for big data processing and analytics.\n",
        "\n",
        "**xgboost:** This stands for eXtreme Gradient Boosting, a scalable and accurate implementation of gradient boosting machines."
      ],
      "metadata": {
        "id": "SGGb2DvjGqLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Understanding PySpark**\n",
        "\n",
        "**What is PySpark?**\n",
        "\n",
        "\n",
        "PySpark allows you to interface with Apache Spark through Python. Apache Spark is a fast, distributed processing system that enables efficient processing of large datasets."
      ],
      "metadata": {
        "id": "Kl73adfeHFUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding XGBoost**\n",
        "\n",
        "**What is XGBoost?**\n",
        "\n",
        "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems."
      ],
      "metadata": {
        "id": "jiBdsvZKHkAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sparkxgb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g5vuyKaIZRi",
        "outputId": "e5e4d9fe-37e6-470c-d002-a9298bbce25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparkxgb\n",
            "  Downloading sparkxgb-0.1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyspark==3.1.1 (from sparkxgb)\n",
            "  Downloading pyspark-3.1.1.tar.gz (212.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9 (from pyspark==3.1.1->sparkxgb)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sparkxgb, pyspark\n",
            "  Building wheel for sparkxgb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sparkxgb: filename=sparkxgb-0.1-py3-none-any.whl size=5629 sha256=4d96ca8ebbc3ad4cfb6c8e50951de1fc582d6e6d9c2117d211b539860cf01518\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0c/a1/786408e13056fabeb8a72134e101b1e142fc95905c7b0e2a71\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767581 sha256=83b3f92e2144ff0c5edbd5f221a73c608596410c88599892463db5687f492367\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/3f/72/8efd988f9ae041f051c75e6834cd92dd6d13a726e206e8b6f3\n",
            "Successfully built sparkxgb pyspark\n",
            "Installing collected packages: py4j, pyspark, sparkxgb\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1 sparkxgb-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By running pip install sparkxgb, you install the library that integrates XGBoost with Spark, allowing you to leverage the distributed computing power of Spark for training XGBoost models.\n",
        "\n",
        "The example demonstrates the steps to create a Spark session, define and train an XGBoost model, and make predictions on a dataset."
      ],
      "metadata": {
        "id": "FJz8XqJXHubF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding sparkxgb\n",
        "\n",
        "sparkxgb integrates XGBoost with Spark, enabling you to use the powerful machine learning capabilities of XGBoost within the distributed computing environment of Spark. This can be particularly useful when working with large datasets that do not fit into the memory of a single machine."
      ],
      "metadata": {
        "id": "4_QPRfb7Ibvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29jT2Gwi4A7P",
        "outputId": "23f2e403-b1b4-4048-ffef-f81e11e6599a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/431.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The emoji library is a handy tool for working with emojis in Python. By installing the library using pip install emoji, you gain access to functions that allow you to add emojis to strings, replace emoji aliases, convert emojis back to their aliases, and check for the presence of emojis in strings.\n",
        "\n",
        " This can be particularly useful for creating more engaging and expressive text in applications such as chatbots, social media tools, and more."
      ],
      "metadata": {
        "id": "0y34XqjVI5DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVlTwXBzyeUJ",
        "outputId": "ebdde9d2-b4a5-4ce7-e87b-15650663223a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line imports the'drive' module from the google.colab package. The google.colab package provides utilities for using Google Colab, and the drive module within it specifically deals with Google Drive integration.\n",
        "\n",
        "This line mounts your Google Drive to the specified directory (/'content/drive') within the Colab environment."
      ],
      "metadata": {
        "id": "eswZqRdGJaU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col , count, when\n",
        "from pyspark.ml.feature import CountVectorizer, StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"SarcasmDetection\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = spark.read.csv(\"/content/drive/MyDrive/iSarcasm/isarcasm.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Display the schema\n",
        "df.printSchema()\n",
        "\n",
        "# Display the data\n",
        "df.show()"
      ],
      "metadata": {
        "id": "o7Ku1PadfmNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0b2308-f77c-46e2-d9ce-f5093870d4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- tweet: string (nullable = true)\n",
            " |-- sarcastic: string (nullable = true)\n",
            " |-- rephrase: string (nullable = true)\n",
            " |-- sarcasm: string (nullable = true)\n",
            " |-- irony: string (nullable = true)\n",
            " |-- satire: string (nullable = true)\n",
            " |-- understatement: string (nullable = true)\n",
            " |-- overstatement: string (nullable = true)\n",
            " |-- rhetorical_question: string (nullable = true)\n",
            "\n",
            "+---+--------------------+--------------------+--------------------+--------------------+-----+------+--------------+-------------+-------------------+\n",
            "|_c0|               tweet|           sarcastic|            rephrase|             sarcasm|irony|satire|understatement|overstatement|rhetorical_question|\n",
            "+---+--------------------+--------------------+--------------------+--------------------+-----+------+--------------+-------------+-------------------+\n",
            "|  0|The only thing I ...|                   1|College is really...|                   0|    1|     0|             0|            0|                  0|\n",
            "|  1|I love it when pr...|                   1|I do not like whe...|                   1|    0|     0|             0|            0|                  0|\n",
            "|  2|Remember the hund...|                   1|I, at the bare mi...|                   0|    1|     0|             0|            0|                  0|\n",
            "|  3|Today my pop-pop ...|                   1|\"Today my pop-pop...|                   1|    0|     0|             0|            0|                  0|\n",
            "|  4|@VolphanCarol @li...|                   1|I would say Ted C...|                   1|    0|     0|             0|            0|                  0|\n",
            "|  5|\"@jimrossignol I ...| poor folks in Ub...|                   1|It's a terrible n...|    0|     1|             0|            1|                  0|\n",
            "|  6|Why would Alexa's...|                   1|Great recipe from...|                   0|    1|     0|             0|            0|                  1|\n",
            "|  7|someone hit me w ...|                   1|Simply “I’m miser...|                   1|    0|     0|             0|            0|                  0|\n",
            "|  8|Loving season 4 o...|                   1|this last year of...|                   1|    0|     0|             0|            0|                  0|\n",
            "|  9|Holly Arnold ??? ...|                   1|Holly Arnold seem...|                   1|    0|     0|             0|            0|                  1|\n",
            "| 10|ANY PENSIONER AND...|                   1|Had a great time ...|                   0|    1|     0|             0|            0|                  0|\n",
            "| 11|See Brexit is goi...|                   1|Brexit really isn...|                   0|    1|     0|             0|            0|                  0|\n",
            "| 12|Just like to cong...|                   1|Can we not sing p...|                   1|    0|     0|             0|            0|                  0|\n",
            "| 13|do i just blast m...|                   1|I probably should...|                   1|    0|     0|             0|            0|                  1|\n",
            "| 14|@heathoween @tyle...|                   1|\"\"\"Not every cele...|                   1|    0|     0|             0|            0|                  0|\n",
            "| 15|I never thought I...|                   1|No, it would be l...|                   1|    0|     0|             0|            0|                  0|\n",
            "| 16|My eldest is havi...|                   1|My eldest is goin...|                   1|    1|     0|             0|            0|                  0|\n",
            "| 17|Whoever’s toddler...|                   1|I hope whoever ke...|                   1|    0|     0|             0|            0|                  0|\n",
            "| 18|gaslight gatekeep...|                   1|I do not gaslight...|                   0|    1|     0|             0|            0|                  0|\n",
            "| 19|I was only in Tay...|                   1|I am not happy th...|                   1|    0|     0|             0|            0|                  0|\n",
            "+---+--------------------+--------------------+--------------------+--------------------+-----+------+--------------+-------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark Libraries:**\n",
        "\n",
        "**SparkSession:** Entry point to Spark functionality.\n",
        "col, count, when: Functions for DataFrame operations.\n",
        "CountVectorizer, StringIndexer, VectorAssembler: Machine learning feature transformers.\n",
        "\n",
        "**Pipeline:** For creating ML pipelines.\n",
        "Other Libraries:\n",
        "\n",
        "**pandas:** Data manipulation library.\n",
        "\n",
        "**xgboost:**  Machine learning library for gradient boosting."
      ],
      "metadata": {
        "id": "V9ENGPyAJ-Tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This line initializes a Spark session named \"SarcasmDetection\". It is necessary to interact with Spark's APIs.\n"
      ],
      "metadata": {
        "id": "YPBwO3QxKsj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This reads a **CSV file** (isarcasm.csv) from Google Drive into a Spark DataFrame.\n",
        "\n",
        "* **header=True:** Indicates that the first row of the CSV file contains column names.\n",
        "\n",
        "* **inferSchema=True:** Automatically infers the data types of the columns."
      ],
      "metadata": {
        "id": "u_7J7dYWK2dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df.printSchema():**Prints the schema of the DataFrame to show the structure and data types of each column.\n",
        "\n",
        "**df.show():**Displays the first 20 rows of the DataFrame. This is useful for getting a quick look at the data."
      ],
      "metadata": {
        "id": "K91k3Oe8LE9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3javQsmFb_D",
        "outputId": "578f18bb-b7ea-46f5-9110-19da60f07ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3834"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df.count():** It provides the total number of rows in the DataFrame, which is useful for understanding the size of the dataset."
      ],
      "metadata": {
        "id": "8DGV-eCgLrMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xuV8iMC4u6B",
        "outputId": "e04d32fc-8dfe-4a84-c38b-81c4ee91620d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_c0',\n",
              " 'tweet',\n",
              " 'sarcastic',\n",
              " 'rephrase',\n",
              " 'sarcasm',\n",
              " 'irony',\n",
              " 'satire',\n",
              " 'understatement',\n",
              " 'overstatement',\n",
              " 'rhetorical_question']"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df.columns:** It provides a list of all column names in the DataFrame, which helps in understanding the structure of your dataset."
      ],
      "metadata": {
        "id": "ayr6z3toMJ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_counts = df.agg(*[count(when(col(i).isNull(), i)).alias(i) for i in df.columns])\n",
        "\n",
        "null_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgO-b5MfTDbo",
        "outputId": "b1616700-12d6-4535-c79c-fafdc859fbc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---------+--------+-------+-----+------+--------------+-------------+-------------------+\n",
            "|_c0|tweet|sarcastic|rephrase|sarcasm|irony|satire|understatement|overstatement|rhetorical_question|\n",
            "+---+-----+---------+--------+-------+-----+------+--------------+-------------+-------------------+\n",
            "|  0|  116|      495|    2932|   2959| 2964|  2966|          2971|         2971|               3012|\n",
            "+---+-----+---------+--------+-------+-----+------+--------------+-------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df.columns:** Retrieves a list of all column names in the DataFrame.\n",
        "col(i).isNull(): Checks if a column i has null values.\n",
        "\n",
        "**when(col(i).isNull(), i): **Creates a condition to identify null values in column i.\n",
        "\n",
        "**count(when(col(i).isNull(), i)):** Counts the number of null values in column i.\n",
        "\n",
        "**alias(i):** Renames the result of the count operation to the column name i.\n",
        "\n",
        "[count(when(col(i).isNull(), i)).alias(i) for i in df.columns]: Creates a list of expressions to count null values for each column.\n",
        "\n",
        "df.agg(*...):Aggregates these expressions over the DataFrame to get the count of null values for each column."
      ],
      "metadata": {
        "id": "Hd7FH2QaMY8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('_c0','rephrase','overstatement','understatement','rhetorical_question')"
      ],
      "metadata": {
        "id": "nGBZpz9_RnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To remove unwanted columns from the DataFrame df.**\n",
        "\n",
        "**Effect:** The DataFrame df will no longer contain the columns '_c0', 'rephrase',' understatement', 'overstatement', and 'rhetorical_question'."
      ],
      "metadata": {
        "id": "aD3eSa5pNMDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.na.drop(how=\"all\")"
      ],
      "metadata": {
        "id": "HbkX1DkGD9hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** To remove rows from the DataFrame where all values are null.\n",
        "\n",
        "**Effect:** Cleans the DataFrame by eliminating rows that do not contain any meaningful data (i.e., rows with all null values)."
      ],
      "metadata": {
        "id": "7J4SBlvsOPIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.na.drop(subset='tweet')"
      ],
      "metadata": {
        "id": "PXPNV6BSFUYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** To remove rows from the DataFrame where the value in the 'tweet' column is null.\n",
        "\n",
        "**Effect:** Ensures that the DataFrame only contains rows where the 'tweet' column has valid (non-null) values."
      ],
      "metadata": {
        "id": "HiAXquZsOUOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.na.drop(subset='sarcastic')"
      ],
      "metadata": {
        "id": "MKaI_50La3G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** To remove rows from the DataFrame where the value in the 'sarcastic' column is null.\n",
        "\n",
        "**Effect:** Ensures that the DataFrame only contains rows where the 'sarcastic' column has valid (non-null) values."
      ],
      "metadata": {
        "id": "BeMbN5ytOmal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.na.fill('0')"
      ],
      "metadata": {
        "id": "sWelnSYex81y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df:** This is your DataFrame that may contain null values.\n",
        "\n",
        "**na:** This accesses the na (null value handling) functions of the DataFrame.\n",
        "\n",
        "**fill('0'):** This function replaces all null values in the DataFrame with the specified value, which in this case is the string '0'."
      ],
      "metadata": {
        "id": "pf42k49m8RsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.na.fill('0')"
      ],
      "metadata": {
        "id": "XNY4fk3XyCge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.na.fill('0')"
      ],
      "metadata": {
        "id": "9AeSTL7xyQFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_counts = df.agg(*[count(when(col(i).isNull(), i)).alias(i) for i in df.columns])\n",
        "\n",
        "null_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ8rsY0RHgsE",
        "outputId": "70783b17-c9ac-4a5c-ad5f-0c6d6fb7fed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-------+-----+------+\n",
            "|tweet|sarcastic|sarcasm|irony|satire|\n",
            "+-----+---------+-------+-----+------+\n",
            "|    0|        0|      0|    0|     0|\n",
            "+-----+---------+-------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df.columns:** Retrieves a list of all column names in the DataFrame.\n",
        "\n",
        "**[count(when(col(i).isNull(), i)).alias(i) for i in df.columns]:** This is a list comprehension that creates a list of aggregation expressions. For each column i:\n",
        "\n",
        "* col(i).isNull(): Checks if the column value is null.\n",
        "\n",
        "* when(col(i).isNull(), i): Returns the column name if the value is null.\n",
        "\n",
        "* count(when(col(i).isNull(), i)): Counts the number of null values in the column.\n",
        "\n",
        "* .alias(i): Assigns the count result an alias, which is the column name.\n",
        "\n",
        "df.agg(*[...]): **bold text** Aggregates the DataFrame based on the list of aggregation expressions.\n",
        "\n",
        "**null_counts:** The resulting DataFrame containing the count of null values for each column.\n",
        "\n",
        "**null_counts.show():** Displays the DataFrame with null counts for each column."
      ],
      "metadata": {
        "id": "Fs93GqxeO0EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn7Et7hlanzQ",
        "outputId": "7429220a-bec8-4014-a2a1-9cf3ea65e14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3338"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract, col\n",
        "\n",
        "# Regex pattern for matching emojis\n",
        "emoji_pattern = \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+\"\n",
        "\n",
        "# Extract emojis from the text column\n",
        "df = df.withColumn(\"emojis\", regexp_extract(col(\"tweet\"), emoji_pattern, 0))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1udOh04yerb",
        "outputId": "4f81a4c5-c063-45d0-9d3d-974ba3929e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----+------+------+\n",
            "|               tweet|           sarcastic|             sarcasm|irony|satire|emojis|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+\n",
            "|The only thing I ...|                   1|                   0|    1|     0|      |\n",
            "|I love it when pr...|                   1|                   1|    0|     0|    ツ|\n",
            "|Remember the hund...|                   1|                   0|    1|     0|🥰🙌🏼|\n",
            "|Today my pop-pop ...|                   1|                   1|    0|     0|    🙃|\n",
            "|@VolphanCarol @li...|                   1|                   1|    0|     0|      |\n",
            "|\"@jimrossignol I ...| poor folks in Ub...|It's a terrible n...|    0|     1|      |\n",
            "|Why would Alexa's...|                   1|                   0|    1|     0|      |\n",
            "|someone hit me w ...|                   1|                   1|    0|     0|      |\n",
            "|Loving season 4 o...|                   1|                   1|    0|     0|      |\n",
            "|Holly Arnold ??? ...|                   1|                   1|    0|     0|      |\n",
            "|ANY PENSIONER AND...|                   1|                   0|    1|     0|      |\n",
            "|See Brexit is goi...|                   1|                   0|    1|     0|      |\n",
            "|Just like to cong...|                   1|                   1|    0|     0|      |\n",
            "|do i just blast m...|                   1|                   1|    0|     0|      |\n",
            "|@heathoween @tyle...|                   1|                   1|    0|     0|      |\n",
            "|I never thought I...|                   1|                   1|    0|     0|      |\n",
            "|My eldest is havi...|                   1|                   1|    1|     0|    😂|\n",
            "|Whoever’s toddler...|                   1|                   1|    0|     0|      |\n",
            "|gaslight gatekeep...|                   1|                   0|    1|     0|      |\n",
            "|I was only in Tay...|                   1|                   1|    0|     0|      |\n",
            "+--------------------+--------------------+--------------------+-----+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.withColumn(\"emojis\", regexp_extract(col(\"tweet\"), emoji_pattern, 0)):\n",
        "\n",
        "Adds a new column named \"emojis\" to the DataFrame df.\n",
        "Uses regexp_extract to search for occurrences of the emoji_pattern in the \"tweet\" column (col(\"tweet\")).\n",
        "\n",
        "The 0 parameter specifies to extract the entire matched substring (the emoji sequence) from the \"tweet\" column.\n",
        "\n",
        "Results are stored in the new \"emojis\" column."
      ],
      "metadata": {
        "id": "xB_RDzHuUL7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define a UDF that uses the emoji library to convert emojis to text\n",
        "def emoji_to_text(text):\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "emoji_to_text_udf = udf(emoji_to_text, StringType())\n",
        "\n",
        "# Apply the UDF to your DataFrame column\n",
        "df = df.withColumn(\"text\", emoji_to_text_udf(\"tweet\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIzOoTN43ouL",
        "outputId": "c3bfe8bf-30ca-4fee-8242-8174b44f950e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+\n",
            "|               tweet|           sarcastic|             sarcasm|irony|satire|emojis|                text|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+\n",
            "|The only thing I ...|                   1|                   0|    1|     0|      |The only thing I ...|\n",
            "|I love it when pr...|                   1|                   1|    0|     0|    ツ|I love it when pr...|\n",
            "|Remember the hund...|                   1|                   0|    1|     0|🥰🙌🏼|Remember the hund...|\n",
            "|Today my pop-pop ...|                   1|                   1|    0|     0|    🙃|Today my pop-pop ...|\n",
            "|@VolphanCarol @li...|                   1|                   1|    0|     0|      |@VolphanCarol @li...|\n",
            "|\"@jimrossignol I ...| poor folks in Ub...|It's a terrible n...|    0|     1|      |\"@jimrossignol I ...|\n",
            "|Why would Alexa's...|                   1|                   0|    1|     0|      |Why would Alexa's...|\n",
            "|someone hit me w ...|                   1|                   1|    0|     0|      |someone hit me w ...|\n",
            "|Loving season 4 o...|                   1|                   1|    0|     0|      |Loving season 4 o...|\n",
            "|Holly Arnold ??? ...|                   1|                   1|    0|     0|      |Holly Arnold ??? ...|\n",
            "|ANY PENSIONER AND...|                   1|                   0|    1|     0|      |ANY PENSIONER AND...|\n",
            "|See Brexit is goi...|                   1|                   0|    1|     0|      |See Brexit is goi...|\n",
            "|Just like to cong...|                   1|                   1|    0|     0|      |Just like to cong...|\n",
            "|do i just blast m...|                   1|                   1|    0|     0|      |do i just blast m...|\n",
            "|@heathoween @tyle...|                   1|                   1|    0|     0|      |@heathoween @tyle...|\n",
            "|I never thought I...|                   1|                   1|    0|     0|      |I never thought I...|\n",
            "|My eldest is havi...|                   1|                   1|    1|     0|    😂|My eldest is havi...|\n",
            "|Whoever’s toddler...|                   1|                   1|    0|     0|      |Whoever’s toddler...|\n",
            "|gaslight gatekeep...|                   1|                   0|    1|     0|      |gaslight gatekeep...|\n",
            "|I was only in Tay...|                   1|                   1|    0|     0|      |I was only in Tay...|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**udf:** Stands for User Defined Function. It allows you to define custom functions that can be applied to DataFrame columns.\n",
        "\n",
        "**StringType:** Represents string data type in PySpark."
      ],
      "metadata": {
        "id": "cI8dzQK5Ulvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**emoji_to_text:** Python function that takes text as input and uses emoji.demojize to convert emojis in text to their textual representation.\n",
        "\n",
        "**udf(emoji_to_text, StringType()):** Creates a PySpark UDF (UserDefinedFunction) named emoji_to_text_udf. It specifies that the output type is StringType."
      ],
      "metadata": {
        "id": "bQUF9xSaVgkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.withColumn(\"text\", emoji_to_text_udf(\"tweet\")): Creates a new column \"text\" in the DataFrame df.\n",
        "\n",
        "Applies the emoji_to_text_udf UDF to the \"tweet\" column of df, converting emojis to their textual representation in the new \"text\" column."
      ],
      "metadata": {
        "id": "cAsQ8NHTVptj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Regex pattern for matching all special characters\n",
        "special_char_pattern = \"[^a-zA-Z0-9\\s]\"\n",
        "\n",
        "# Remove special characters from the text column\n",
        "df = df.withColumn(\"clean_text\", regexp_replace(col(\"text\"), special_char_pattern, \"\"))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2Q3LFgBSbV",
        "outputId": "58367381-3a04-4f96-d21a-4bee2630f195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+\n",
            "|               tweet|           sarcastic|             sarcasm|irony|satire|emojis|                text|          clean_text|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+\n",
            "|The only thing I ...|                   1|                   0|    1|     0|      |The only thing I ...|The only thing I ...|\n",
            "|I love it when pr...|                   1|                   1|    0|     0|    ツ|I love it when pr...|I love it when pr...|\n",
            "|Remember the hund...|                   1|                   0|    1|     0|🥰🙌🏼|Remember the hund...|Remember the hund...|\n",
            "|Today my pop-pop ...|                   1|                   1|    0|     0|    🙃|Today my pop-pop ...|Today my poppop t...|\n",
            "|@VolphanCarol @li...|                   1|                   1|    0|     0|      |@VolphanCarol @li...|VolphanCarol litt...|\n",
            "|\"@jimrossignol I ...| poor folks in Ub...|It's a terrible n...|    0|     1|      |\"@jimrossignol I ...|jimrossignol I ch...|\n",
            "|Why would Alexa's...|                   1|                   0|    1|     0|      |Why would Alexa's...|Why would Alexas ...|\n",
            "|someone hit me w ...|                   1|                   1|    0|     0|      |someone hit me w ...|someone hit me w ...|\n",
            "|Loving season 4 o...|                   1|                   1|    0|     0|      |Loving season 4 o...|Loving season 4 o...|\n",
            "|Holly Arnold ??? ...|                   1|                   1|    0|     0|      |Holly Arnold ??? ...|Holly Arnold  Who...|\n",
            "|ANY PENSIONER AND...|                   1|                   0|    1|     0|      |ANY PENSIONER AND...|ANY PENSIONER AND...|\n",
            "|See Brexit is goi...|                   1|                   0|    1|     0|      |See Brexit is goi...|See Brexit is goi...|\n",
            "|Just like to cong...|                   1|                   1|    0|     0|      |Just like to cong...|Just like to cong...|\n",
            "|do i just blast m...|                   1|                   1|    0|     0|      |do i just blast m...|do i just blast m...|\n",
            "|@heathoween @tyle...|                   1|                   1|    0|     0|      |@heathoween @tyle...|heathoween tylerr...|\n",
            "|I never thought I...|                   1|                   1|    0|     0|      |I never thought I...|I never thought I...|\n",
            "|My eldest is havi...|                   1|                   1|    1|     0|    😂|My eldest is havi...|My eldest is havi...|\n",
            "|Whoever’s toddler...|                   1|                   1|    0|     0|      |Whoever’s toddler...|Whoevers toddler ...|\n",
            "|gaslight gatekeep...|                   1|                   0|    1|     0|      |gaslight gatekeep...|gaslight gatekeep...|\n",
            "|I was only in Tay...|                   1|                   1|    0|     0|      |I was only in Tay...|I was only in Tay...|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**regexp_replace:** PySpark SQL function used to replace substrings that match a regex pattern with a specified string.\n",
        "\n",
        "**special_char_pattern:** Regular expression pattern that matches all characters except letters ('a-zA-Z'), digits ('0-9'), and whitespace ('\\s').\n",
        "\n",
        "**df.withColumn**(\"clean_text\", regexp_replace(col(\"text\"), special_char_pattern, \"\")):\n",
        "\n",
        "**Creates a new column** \"clean_text\" in the DataFrame df.\n",
        "\n",
        "Applies regexp_replace to the \"text\" column (col(\"text\")), replacing all substrings that match special_char_pattern with an empty string (\"\").\n",
        "\n",
        "**Stores** the result in the new column \"clean_text\"."
      ],
      "metadata": {
        "id": "lXMK0JTeV30m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# Tokenize the text column into words\n",
        "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
        "df = tokenizer.transform(df)\n",
        "\n",
        "# Remove stopwords\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "df = remover.transform(df)\n",
        "\n",
        "# Show the resulting DataFrame with stopwords removed\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJlGnyQt-Rw4",
        "outputId": "efa6f616-9169-4da3-d7b4-a787d7482c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+--------------------+--------------------+\n",
            "|               tweet|           sarcastic|             sarcasm|irony|satire|emojis|                text|          clean_text|               words|            filtered|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+--------------------+--------------------+\n",
            "|The only thing I ...|                   1|                   0|    1|     0|      |The only thing I ...|The only thing I ...|[the, only, thing...|[thing, got, coll...|\n",
            "|I love it when pr...|                   1|                   1|    0|     0|    ツ|I love it when pr...|I love it when pr...|[i, love, it, whe...|[love, professors...|\n",
            "|Remember the hund...|                   1|                   0|    1|     0|🥰🙌🏼|Remember the hund...|Remember the hund...|[remember, the, h...|[remember, hundre...|\n",
            "|Today my pop-pop ...|                   1|                   1|    0|     0|    🙃|Today my pop-pop ...|Today my poppop t...|[today, my, poppo...|[today, poppop, t...|\n",
            "|@VolphanCarol @li...|                   1|                   1|    0|     0|      |@VolphanCarol @li...|VolphanCarol litt...|[volphancarol, li...|[volphancarol, li...|\n",
            "|\"@jimrossignol I ...| poor folks in Ub...|It's a terrible n...|    0|     1|      |\"@jimrossignol I ...|jimrossignol I ch...|[jimrossignol, i,...|[jimrossignol, ch...|\n",
            "|Why would Alexa's...|                   1|                   0|    1|     0|      |Why would Alexa's...|Why would Alexas ...|[why, would, alex...|[alexas, recipe, ...|\n",
            "|someone hit me w ...|                   1|                   1|    0|     0|      |someone hit me w ...|someone hit me w ...|[someone, hit, me...|[someone, hit, w,...|\n",
            "|Loving season 4 o...|                   1|                   1|    0|     0|      |Loving season 4 o...|Loving season 4 o...|[loving, season, ...|[loving, season, ...|\n",
            "|Holly Arnold ??? ...|                   1|                   1|    0|     0|      |Holly Arnold ??? ...|Holly Arnold  Who...|[holly, arnold, ,...|[holly, arnold, ,...|\n",
            "|ANY PENSIONER AND...|                   1|                   0|    1|     0|      |ANY PENSIONER AND...|ANY PENSIONER AND...|[any, pensioner, ...|[pensioner, 4, ye...|\n",
            "|See Brexit is goi...|                   1|                   0|    1|     0|      |See Brexit is goi...|See Brexit is goi...|[see, brexit, is,...|[see, brexit, goi...|\n",
            "|Just like to cong...|                   1|                   1|    0|     0|      |Just like to cong...|Just like to cong...|[just, like, to, ...|[like, congratula...|\n",
            "|do i just blast m...|                   1|                   1|    0|     0|      |do i just blast m...|do i just blast m...|[do, i, just, bla...|[blast, maneskin,...|\n",
            "|@heathoween @tyle...|                   1|                   1|    0|     0|      |@heathoween @tyle...|heathoween tylerr...|[heathoween, tyle...|[heathoween, tyle...|\n",
            "|I never thought I...|                   1|                   1|    0|     0|      |I never thought I...|I never thought I...|[i, never, though...|[never, thought, ...|\n",
            "|My eldest is havi...|                   1|                   1|    1|     0|    😂|My eldest is havi...|My eldest is havi...|[my, eldest, is, ...|[eldest, wild, fr...|\n",
            "|Whoever’s toddler...|                   1|                   1|    0|     0|      |Whoever’s toddler...|Whoevers toddler ...|[whoevers, toddle...|[whoevers, toddle...|\n",
            "|gaslight gatekeep...|                   1|                   0|    1|     0|      |gaslight gatekeep...|gaslight gatekeep...|[gaslight, gateke...|[gaslight, gateke...|\n",
            "|I was only in Tay...|                   1|                   1|    0|     0|      |I was only in Tay...|I was only in Tay...|[i, was, only, in...|[taylor, swifts, ...|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer:** A feature transformer in PySpark ML that splits text into words (tokens) based on whitespace or specified patterns.\n",
        "\n",
        "**StopWordsRemover: **A feature transformer in PySpark ML that removes common words (stopwords) from a sequence of words."
      ],
      "metadata": {
        "id": "-fbEKk-LW0fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer(inputCol=\"clean_text\", outputCol=\"words\"):**\n",
        "Creates a Tokenizer instance that will tokenize the \"clean_text\" column (inputCol=\"clean_text\").\n",
        "\n",
        "Outputs the tokens into a new column named \"words\" (outputCol=\"words\").\n",
        "\n",
        "**tokenizer.transform(df):** Transforms the DataFrame df by applying the Tokenizer to create a new column \"words\" containing tokenized text."
      ],
      "metadata": {
        "id": "JCLc0qW4bf6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\"):\n",
        "Creates a StopWordsRemover instance that removes stopwords from the \"words\" column (inputCol=\"words\").\n",
        "* Outputs the filtered words into a new column named \"filtered\" (outputCol=\"filtered\").\n",
        "\n",
        "**remover.transform(df):** Transforms the DataFrame df by applying the StopWordsRemover to create a new column \"filtered\" containing words with stopwords removed."
      ],
      "metadata": {
        "id": "rqewzH_Jbrot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stringIndexer1 = StringIndexer(inputCol=\"sarcastic\", outputCol=\"label\")\n",
        "stringIndexer2 = StringIndexer(inputCol=\"sarcasm\", outputCol=\"newsarcasm\")\n",
        "stringIndexer3 = StringIndexer(inputCol=\"irony\", outputCol=\"newirony\")\n",
        "stringIndexer4 = StringIndexer(inputCol=\"satire\", outputCol=\"newsatire\")\n",
        "countVectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"features\",\"newsarcasm\",\"newirony\",\"newsatire\"], outputCol=\"combinedFeatures\")\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline(stages=[stringIndexer1, stringIndexer2, stringIndexer3, stringIndexer4, countVectorizer, vectorAssembler])\n",
        "\n",
        "# Fit the pipeline to the DataFrame\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Transform the DataFrame\n",
        "df = model.transform(df)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez-MlQp-IoQ-",
        "outputId": "2fe64002-2a67-4948-fb9c-f83deea1f5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+--------------------+--------------------+-----+----------+--------+---------+--------------------+--------------------+\n",
            "|               tweet|           sarcastic|             sarcasm|irony|satire|emojis|                text|          clean_text|               words|            filtered|label|newsarcasm|newirony|newsatire|            features|    combinedFeatures|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+--------------------+--------------------+-----+----------+--------+---------+--------------------+--------------------+\n",
            "|The only thing I ...|                   1|                   0|    1|     0|      |The only thing I ...|The only thing I ...|[the, only, thing...|[thing, got, coll...|  1.0|       0.0|     1.0|      0.0|(10223,[19,42,419...|(10226,[19,42,419...|\n",
            "|I love it when pr...|                   1|                   1|    0|     0|    ツ|I love it when pr...|I love it when pr...|[i, love, it, whe...|[love, professors...|  1.0|       1.0|     0.0|      0.0|(10223,[0,2,3,8,5...|(10226,[0,2,3,8,5...|\n",
            "|Remember the hund...|                   1|                   0|    1|     0|🥰🙌🏼|Remember the hund...|Remember the hund...|[remember, the, h...|[remember, hundre...|  1.0|       0.0|     1.0|      0.0|(10223,[3,10,43,5...|(10226,[3,10,43,5...|\n",
            "|Today my pop-pop ...|                   1|                   1|    0|     0|    🙃|Today my pop-pop ...|Today my poppop t...|[today, my, poppo...|[today, poppop, t...|  1.0|       1.0|     0.0|      0.0|(10223,[14,20,116...|(10226,[14,20,116...|\n",
            "|@VolphanCarol @li...|                   1|                   1|    0|     0|      |@VolphanCarol @li...|VolphanCarol litt...|[volphancarol, li...|[volphancarol, li...|  1.0|       1.0|     0.0|      0.0|(10223,[52,106,10...|(10226,[52,106,10...|\n",
            "|\"@jimrossignol I ...| poor folks in Ub...|It's a terrible n...|    0|     1|      |\"@jimrossignol I ...|jimrossignol I ch...|[jimrossignol, i,...|[jimrossignol, ch...| 28.0|      28.0|     0.0|      1.0|(10223,[684,953,1...|(10226,[684,953,1...|\n",
            "|Why would Alexa's...|                   1|                   0|    1|     0|      |Why would Alexa's...|Why would Alexas ...|[why, would, alex...|[alexas, recipe, ...|  1.0|       0.0|     1.0|      0.0|(10223,[1,1789,34...|(10226,[1,1789,34...|\n",
            "|someone hit me w ...|                   1|                   1|    0|     0|      |someone hit me w ...|someone hit me w ...|[someone, hit, me...|[someone, hit, w,...|  1.0|       1.0|     0.0|      0.0|(10223,[0,3,10,17...|(10226,[0,3,10,17...|\n",
            "|Loving season 4 o...|                   1|                   1|    0|     0|      |Loving season 4 o...|Loving season 4 o...|[loving, season, ...|[loving, season, ...|  1.0|       1.0|     0.0|      0.0|(10223,[140,151,2...|(10226,[140,151,2...|\n",
            "|Holly Arnold ??? ...|                   1|                   1|    0|     0|      |Holly Arnold ??? ...|Holly Arnold  Who...|[holly, arnold, ,...|[holly, arnold, ,...|  1.0|       1.0|     0.0|      0.0|(10223,[1,93,124,...|(10226,[1,93,124,...|\n",
            "|ANY PENSIONER AND...|                   1|                   0|    1|     0|      |ANY PENSIONER AND...|ANY PENSIONER AND...|[any, pensioner, ...|[pensioner, 4, ye...|  1.0|       0.0|     1.0|      0.0|(10223,[25,41,74,...|(10226,[25,41,74,...|\n",
            "|See Brexit is goi...|                   1|                   0|    1|     0|      |See Brexit is goi...|See Brexit is goi...|[see, brexit, is,...|[see, brexit, goi...|  1.0|       0.0|     1.0|      0.0|(10223,[16,23,68,...|(10226,[16,23,68,...|\n",
            "|Just like to cong...|                   1|                   1|    0|     0|      |Just like to cong...|Just like to cong...|[just, like, to, ...|[like, congratula...|  1.0|       1.0|     0.0|      0.0|(10223,[2,14,44,5...|(10226,[2,14,44,5...|\n",
            "|do i just blast m...|                   1|                   1|    0|     0|      |do i just blast m...|do i just blast m...|[do, i, just, bla...|[blast, maneskin,...|  1.0|       1.0|     0.0|      0.0|(10223,[6,1850,18...|(10226,[6,1850,18...|\n",
            "|@heathoween @tyle...|                   1|                   1|    0|     0|      |@heathoween @tyle...|heathoween tylerr...|[heathoween, tyle...|[heathoween, tyle...|  1.0|       1.0|     0.0|      0.0|(10223,[6,18,27,3...|(10226,[6,18,27,3...|\n",
            "|I never thought I...|                   1|                   1|    0|     0|      |I never thought I...|I never thought I...|[i, never, though...|[never, thought, ...|  1.0|       1.0|     0.0|      0.0|(10223,[2,5,7,28,...|(10226,[2,5,7,28,...|\n",
            "|My eldest is havi...|                   1|                   1|    1|     0|    😂|My eldest is havi...|My eldest is havi...|[my, eldest, is, ...|[eldest, wild, fr...|  1.0|       1.0|     1.0|      0.0|(10223,[16,63,233...|(10226,[16,63,233...|\n",
            "|Whoever’s toddler...|                   1|                   1|    0|     0|      |Whoever’s toddler...|Whoevers toddler ...|[whoevers, toddle...|[whoevers, toddle...|  1.0|       1.0|     0.0|      0.0|(10223,[2,11,20,5...|(10226,[2,11,20,5...|\n",
            "|gaslight gatekeep...|                   1|                   0|    1|     0|      |gaslight gatekeep...|gaslight gatekeep...|[gaslight, gateke...|[gaslight, gateke...|  1.0|       0.0|     1.0|      0.0|(10223,[82,686,25...|(10226,[82,686,25...|\n",
            "|I was only in Tay...|                   1|                   1|    0|     0|      |I was only in Tay...|I was only in Tay...|[i, was, only, in...|[taylor, swifts, ...|  1.0|       1.0|     0.0|      0.0|(10223,[25,37,321...|(10226,[25,37,321...|\n",
            "+--------------------+--------------------+--------------------+-----+------+------+--------------------+--------------------+--------------------+--------------------+-----+----------+--------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StringIndexer:** A feature transformer in PySpark ML that indexes categorical labels into numerical labels.\n",
        "\n",
        "**CountVectorizer:** A feature transformer in PySpark ML that converts a collection of text documents into a sparse matrix of token counts.\n",
        "\n",
        "**VectorAssembler:** A transformer in PySpark ML that combines multiple columns into a single vector column.\n",
        "\n",
        "**Pipeline:** A sequence of stages to process and learn from data in a structured way.\n",
        "\n",
        "Creates a CountVectorizer instance to convert the \"filtered\" column (containing words after stopwords removal) into a feature vector.\n",
        "\n",
        "Outputs a sparse vector representation of word counts into a new column named \"features\".\n",
        "\n",
        "**Fits the defined pipeline to the DataFrame df, applying each stage sequentially.**\n",
        "\n",
        "Transforms the DataFrame df using the fitted pipeline (model), applying all stages (StringIndexer, CountVectorizer, VectorAssembler)."
      ],
      "metadata": {
        "id": "J499ibc7b9lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aChZOm3chMs",
        "outputId": "45bad278-cb67-418f-af9f-6f7249b311d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- tweet: string (nullable = false)\n",
            " |-- sarcastic: string (nullable = false)\n",
            " |-- sarcasm: string (nullable = false)\n",
            " |-- irony: string (nullable = false)\n",
            " |-- satire: string (nullable = false)\n",
            " |-- emojis: string (nullable = false)\n",
            " |-- text: string (nullable = true)\n",
            " |-- clean_text: string (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- filtered: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- label: double (nullable = false)\n",
            " |-- newsarcasm: double (nullable = false)\n",
            " |-- newirony: double (nullable = false)\n",
            " |-- newsatire: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- combinedFeatures: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** To understand the structure of the DataFrame df and its columns.\n",
        "\n",
        "**Output:** Describes each column with its name, data type, and nullability."
      ],
      "metadata": {
        "id": "DmmQpuooggid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.filter(df.label <= 1)\n",
        "df=df.filter(df.newsarcasm <= 1)\n",
        "df=df.filter(df.newirony <= 1)\n",
        "df=df.filter(df.newsatire <= 1)"
      ],
      "metadata": {
        "id": "PJ7cPQSxWNOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: To filter rows in the DataFrame based on a condition involving the \"label\", \"newsarcasm\", \"newirony\", \"newsatire\" column."
      ],
      "metadata": {
        "id": "ZM5-jgoegs6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V7Elp__hw90",
        "outputId": "4e702532-70cb-4c6f-a332-104dea74ad81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3230"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrAOIetnh1Np",
        "outputId": "1b6b4a70-fe1d-4086-f96a-00605bf4e004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+-------+-----+------+------+--------------------+--------------------+--------------------+--------------------+-----+----------+--------+---------+--------------------+--------------------+\n",
            "|               tweet|sarcastic|sarcasm|irony|satire|emojis|                text|          clean_text|               words|            filtered|label|newsarcasm|newirony|newsatire|            features|    combinedFeatures|\n",
            "+--------------------+---------+-------+-----+------+------+--------------------+--------------------+--------------------+--------------------+-----+----------+--------+---------+--------------------+--------------------+\n",
            "|The only thing I ...|        1|      0|    1|     0|      |The only thing I ...|The only thing I ...|[the, only, thing...|[thing, got, coll...|  1.0|       0.0|     1.0|      0.0|(10223,[19,42,419...|(10226,[19,42,419...|\n",
            "|I love it when pr...|        1|      1|    0|     0|    ツ|I love it when pr...|I love it when pr...|[i, love, it, whe...|[love, professors...|  1.0|       1.0|     0.0|      0.0|(10223,[0,2,3,8,5...|(10226,[0,2,3,8,5...|\n",
            "|Remember the hund...|        1|      0|    1|     0|🥰🙌🏼|Remember the hund...|Remember the hund...|[remember, the, h...|[remember, hundre...|  1.0|       0.0|     1.0|      0.0|(10223,[3,10,43,5...|(10226,[3,10,43,5...|\n",
            "|Today my pop-pop ...|        1|      1|    0|     0|    🙃|Today my pop-pop ...|Today my poppop t...|[today, my, poppo...|[today, poppop, t...|  1.0|       1.0|     0.0|      0.0|(10223,[14,20,116...|(10226,[14,20,116...|\n",
            "|@VolphanCarol @li...|        1|      1|    0|     0|      |@VolphanCarol @li...|VolphanCarol litt...|[volphancarol, li...|[volphancarol, li...|  1.0|       1.0|     0.0|      0.0|(10223,[52,106,10...|(10226,[52,106,10...|\n",
            "|Why would Alexa's...|        1|      0|    1|     0|      |Why would Alexa's...|Why would Alexas ...|[why, would, alex...|[alexas, recipe, ...|  1.0|       0.0|     1.0|      0.0|(10223,[1,1789,34...|(10226,[1,1789,34...|\n",
            "|someone hit me w ...|        1|      1|    0|     0|      |someone hit me w ...|someone hit me w ...|[someone, hit, me...|[someone, hit, w,...|  1.0|       1.0|     0.0|      0.0|(10223,[0,3,10,17...|(10226,[0,3,10,17...|\n",
            "|Loving season 4 o...|        1|      1|    0|     0|      |Loving season 4 o...|Loving season 4 o...|[loving, season, ...|[loving, season, ...|  1.0|       1.0|     0.0|      0.0|(10223,[140,151,2...|(10226,[140,151,2...|\n",
            "|Holly Arnold ??? ...|        1|      1|    0|     0|      |Holly Arnold ??? ...|Holly Arnold  Who...|[holly, arnold, ,...|[holly, arnold, ,...|  1.0|       1.0|     0.0|      0.0|(10223,[1,93,124,...|(10226,[1,93,124,...|\n",
            "|ANY PENSIONER AND...|        1|      0|    1|     0|      |ANY PENSIONER AND...|ANY PENSIONER AND...|[any, pensioner, ...|[pensioner, 4, ye...|  1.0|       0.0|     1.0|      0.0|(10223,[25,41,74,...|(10226,[25,41,74,...|\n",
            "|See Brexit is goi...|        1|      0|    1|     0|      |See Brexit is goi...|See Brexit is goi...|[see, brexit, is,...|[see, brexit, goi...|  1.0|       0.0|     1.0|      0.0|(10223,[16,23,68,...|(10226,[16,23,68,...|\n",
            "|Just like to cong...|        1|      1|    0|     0|      |Just like to cong...|Just like to cong...|[just, like, to, ...|[like, congratula...|  1.0|       1.0|     0.0|      0.0|(10223,[2,14,44,5...|(10226,[2,14,44,5...|\n",
            "|do i just blast m...|        1|      1|    0|     0|      |do i just blast m...|do i just blast m...|[do, i, just, bla...|[blast, maneskin,...|  1.0|       1.0|     0.0|      0.0|(10223,[6,1850,18...|(10226,[6,1850,18...|\n",
            "|@heathoween @tyle...|        1|      1|    0|     0|      |@heathoween @tyle...|heathoween tylerr...|[heathoween, tyle...|[heathoween, tyle...|  1.0|       1.0|     0.0|      0.0|(10223,[6,18,27,3...|(10226,[6,18,27,3...|\n",
            "|I never thought I...|        1|      1|    0|     0|      |I never thought I...|I never thought I...|[i, never, though...|[never, thought, ...|  1.0|       1.0|     0.0|      0.0|(10223,[2,5,7,28,...|(10226,[2,5,7,28,...|\n",
            "|My eldest is havi...|        1|      1|    1|     0|    😂|My eldest is havi...|My eldest is havi...|[my, eldest, is, ...|[eldest, wild, fr...|  1.0|       1.0|     1.0|      0.0|(10223,[16,63,233...|(10226,[16,63,233...|\n",
            "|Whoever’s toddler...|        1|      1|    0|     0|      |Whoever’s toddler...|Whoevers toddler ...|[whoevers, toddle...|[whoevers, toddle...|  1.0|       1.0|     0.0|      0.0|(10223,[2,11,20,5...|(10226,[2,11,20,5...|\n",
            "|gaslight gatekeep...|        1|      0|    1|     0|      |gaslight gatekeep...|gaslight gatekeep...|[gaslight, gateke...|[gaslight, gateke...|  1.0|       0.0|     1.0|      0.0|(10223,[82,686,25...|(10226,[82,686,25...|\n",
            "|I was only in Tay...|        1|      1|    0|     0|      |I was only in Tay...|I was only in Tay...|[i, was, only, in...|[taylor, swifts, ...|  1.0|       1.0|     0.0|      0.0|(10223,[25,37,321...|(10226,[25,37,321...|\n",
            "|Imagine going to ...|        1|      1|    0|     0|      |Imagine going to ...|Imagine going to ...|[imagine, going, ...|[imagine, going, ...|  1.0|       1.0|     0.0|      0.0|(10223,[16,60,140...|(10226,[16,60,140...|\n",
            "+--------------------+---------+-------+-----+------+------+--------------------+--------------------+--------------------+--------------------+-----+----------+--------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"label\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lit70tYY3xp3",
        "outputId": "d0faa4e5-41b1-4932-c0b7-05c5b451ec6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0| 2435|\n",
            "|  1.0|  795|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code df.groupBy(\"label\").count().show() groups the DataFrame df by the label column, counts the rows in each group, and displays the result."
      ],
      "metadata": {
        "id": "_1s72Iuk-V9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"newsarcasm\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWiwB6ve3gFv",
        "outputId": "30447ea0-c2e3-470a-bfcf-c90fa975d4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|newsarcasm|count|\n",
            "+----------+-----+\n",
            "|       0.0| 2578|\n",
            "|       1.0|  652|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code df.groupBy(\"newsarcasm\").count().show() groups the DataFrame df by the newsarcasm column, counts the rows in each group, and displays the result."
      ],
      "metadata": {
        "id": "yzSpoeTS_xSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"newirony\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mha1QFz0bjCU",
        "outputId": "b1966023-a2e0-4e0f-bdea-02c2ac491c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|newirony|count|\n",
            "+--------+-----+\n",
            "|     0.0| 3090|\n",
            "|     1.0|  140|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code df.groupBy(\"newirony\").count().show() groups the DataFrame df by the newirony column, counts the rows in each group, and displays the result."
      ],
      "metadata": {
        "id": "fr_VENIj_-dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"newsatire\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gK0p39i35s8",
        "outputId": "c7de5ee2-45c6-4ecd-bcf6-5af7f1e596ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|newsatire|count|\n",
            "+---------+-----+\n",
            "|      0.0| 3208|\n",
            "|      1.0|   22|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code df.groupBy(\"newsatire\").count().show() groups the DataFrame df by the newsatire column, counts the rows in each group, and displays the result."
      ],
      "metadata": {
        "id": "s80Q5OIUALvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data = df.select(\"combinedFeatures\", \"label\")"
      ],
      "metadata": {
        "id": "RntWEeKIdIno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code finalized_data = df.select(\"combinedFeatures\", \"label\") creates a new DataFrame finalized_data by selecting only the combinedFeatures and label columns from the original DataFrame df."
      ],
      "metadata": {
        "id": "Ano7jyrzhOFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwGBllwUwfN3",
        "outputId": "31612015-0dc9-4e57-a5d1-387d4bcb8767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|    combinedFeatures|label|\n",
            "+--------------------+-----+\n",
            "|(10226,[19,42,419...|  1.0|\n",
            "|(10226,[0,2,3,8,5...|  1.0|\n",
            "|(10226,[3,10,43,5...|  1.0|\n",
            "|(10226,[14,20,116...|  1.0|\n",
            "|(10226,[52,106,10...|  1.0|\n",
            "|(10226,[1,1789,34...|  1.0|\n",
            "|(10226,[0,3,10,17...|  1.0|\n",
            "|(10226,[140,151,2...|  1.0|\n",
            "|(10226,[1,93,124,...|  1.0|\n",
            "|(10226,[25,41,74,...|  1.0|\n",
            "|(10226,[16,23,68,...|  1.0|\n",
            "|(10226,[2,14,44,5...|  1.0|\n",
            "|(10226,[6,1850,18...|  1.0|\n",
            "|(10226,[6,18,27,3...|  1.0|\n",
            "|(10226,[2,5,7,28,...|  1.0|\n",
            "|(10226,[16,63,233...|  1.0|\n",
            "|(10226,[2,11,20,5...|  1.0|\n",
            "|(10226,[82,686,25...|  1.0|\n",
            "|(10226,[25,37,321...|  1.0|\n",
            "|(10226,[16,60,140...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data.groupBy(\"label\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_vbKLWr_ep9",
        "outputId": "00f31c54-96a9-4985-84f4-3b07695f494b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0| 2435|\n",
            "|  1.0|  795|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code finalized_data.groupBy(\"label\").count().show() groups the finalized_data DataFrame by the label column, counts the rows in each group, and displays the result."
      ],
      "metadata": {
        "id": "bwJ9oH7lCIqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "train_data, test_data = finalized_data.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "DMhz1ZWbGMdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code train_data, test_data = finalized_data.randomSplit([0.8, 0.2]) splits the finalized_data DataFrame into two subsets train_data and test_data with approximately 80% of the data assigned to train_data and 20% to test_data, randomly distributed."
      ],
      "metadata": {
        "id": "C0vEvxL3CV3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.groupBy().count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOqARzBv_u4A",
        "outputId": "289ea173-6af8-424c-a8a2-90848b1b2e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|count|\n",
            "+-----+\n",
            "| 2580|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code train_data.groupBy().count().show() groups the train_data DataFrame and counts the total number of rows in the DataFrame, displaying the count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVOeLr-pENpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.groupBy().count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEhAy9pDAg0V",
        "outputId": "0c78c798-a24b-4198-e329-15e8c0a2d6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|count|\n",
            "+-----+\n",
            "|  650|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "tFMkmFbuPvGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas DataFrame\n",
        "train_pd = train_data.select(\"combinedFeatures\", \"label\").toPandas()\n",
        "test_pd = test_data.select(\"combinedFeatures\", \"label\").toPandas()"
      ],
      "metadata": {
        "id": "6Aizd0HEHzKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train_data.select(\"combinedFeatures\", \"label\"):** Selects only the columns \"combinedFeatures\" and \"label\" from the train_data DataFrame.\n",
        "\n",
        "**.toPandas():** Converts the PySpark DataFrame train_data (or test_data in the second line) into a Pandas DataFrame (train_pd or test_pd respectively). This conversion allows for easier handling and analysis using Pandas functionalities, which are commonly used in Python data analysis workflows."
      ],
      "metadata": {
        "id": "LyT9rEkUEZpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.groupBy(\"label\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31J4f03iA3-U",
        "outputId": "06b2282a-cc93-4ea2-f79f-6409a0665adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0| 1928|\n",
            "|  1.0|  652|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.groupBy(\"label\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTOhkQurBMz4",
        "outputId": "e770b0d9-b605-422d-fb91-2e09264511ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|  507|\n",
            "|  1.0|  143|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pd.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0zuDtMcXuf8",
        "outputId": "907ac1dc-3d4f-4cee-d5fd-538fca61047d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['combinedFeatures', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_pd.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd25_R8mZz0X",
        "outputId": "41a8826e-726e-457d-cbe6-6df10ee93886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combinedFeatures     object\n",
            "label               float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'combinedFeatures' to 'category' if it's categorical\n",
        "train_pd['combinedFeatures'] = train_pd['combinedFeatures'].astype('category')\n",
        "test_pd['combinedFeatures'] = test_pd['combinedFeatures'].astype('category')\n",
        "# Check the dtypes again\n",
        "print(train_pd.dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPvatGX8abnH",
        "outputId": "02e841fd-cff7-4341-af04-1c25c543478f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combinedFeatures    category\n",
            "label                float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines convert the column 'combinedFeatures' in both the train_pd and test_pd Pandas DataFrames to a categorical data type. This is useful when 'combinedFeatures' represents categorical data (e.g., predefined categories or labels).\n",
        "\n",
        "After converting 'combinedFeatures' to categorical, this line prints the data types of all columns in the train_pd DataFrame. It helps to verify that 'combinedFeatures' is indeed of type 'category' after the conversion."
      ],
      "metadata": {
        "id": "UbfHM-4TEpaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Pandas equivalent for grouping and counting\n",
        "train_pd.groupby(\"label\").size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEGEOk_0918n",
        "outputId": "51acba96-1fbe-48e9-ba2a-950caf80d087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0.0    1928\n",
              "1.0     652\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42, enable_categorical=True)\n",
        "xgb_model.fit(train_pd.drop('label', axis=1), train_pd['label'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "5H2XQDO_asOI",
        "outputId": "29573422-db06-4d3f-ac7b-0206b9437fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=42, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**xgb.XGBClassifier:** Initializes an instance of the XGBoost classifier.\n",
        "\n",
        "**objective='binary:logistic':** Specifies the objective function for binary classification using logistic regression.\n",
        "\n",
        "**random_state=42:** Sets a seed for reproducibility.\n",
        "\n",
        "**enable_categorical=True:** Enables usage of categorical features in XGBoost (typically useful when features are categorical and have been encoded appropriately).\n",
        "\n",
        "fit(train_pd.drop('label', axis=1), train_pd['label']): Fits (trains) the XGBoost model (xgb_model) using training data.\n",
        "\n",
        "**train_pd.drop('label', axis=1):** Drops the 'label' column from train_pd, leaving only the feature columns.\n",
        "\n",
        "**train_pd['label']:** Provides the target variable (labels) for training, which is expected to be binary (0 or 1)."
      ],
      "metadata": {
        "id": "I_JPURGkh2Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare DMatrix\n",
        "dtrain = xgb.DMatrix(train_pd['combinedFeatures'].tolist(), label=train_pd['label'])\n",
        "dtest = xgb.DMatrix(test_pd['combinedFeatures'].tolist(), label=test_pd['label'])\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'max_depth': 5,\n",
        "    'eta': 0.1,\n",
        "    'eval_metric': 'logloss'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtest, 'test')], early_stopping_rounds=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EaFb8OnbcCz",
        "outputId": "ab4ab263-38d4-40a8-917d-6b665e9739d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttest-logloss:0.45143\n",
            "[1]\ttest-logloss:0.38924\n",
            "[2]\ttest-logloss:0.33937\n",
            "[3]\ttest-logloss:0.29820\n",
            "[4]\ttest-logloss:0.26347\n",
            "[5]\ttest-logloss:0.23372\n",
            "[6]\ttest-logloss:0.20815\n",
            "[7]\ttest-logloss:0.18589\n",
            "[8]\ttest-logloss:0.16644\n",
            "[9]\ttest-logloss:0.14926\n",
            "[10]\ttest-logloss:0.13416\n",
            "[11]\ttest-logloss:0.12079\n",
            "[12]\ttest-logloss:0.10891\n",
            "[13]\ttest-logloss:0.09833\n",
            "[14]\ttest-logloss:0.08890\n",
            "[15]\ttest-logloss:0.08047\n",
            "[16]\ttest-logloss:0.07279\n",
            "[17]\ttest-logloss:0.06590\n",
            "[18]\ttest-logloss:0.05973\n",
            "[19]\ttest-logloss:0.05416\n",
            "[20]\ttest-logloss:0.04916\n",
            "[21]\ttest-logloss:0.04467\n",
            "[22]\ttest-logloss:0.04064\n",
            "[23]\ttest-logloss:0.03696\n",
            "[24]\ttest-logloss:0.03367\n",
            "[25]\ttest-logloss:0.03068\n",
            "[26]\ttest-logloss:0.02797\n",
            "[27]\ttest-logloss:0.02556\n",
            "[28]\ttest-logloss:0.02339\n",
            "[29]\ttest-logloss:0.02142\n",
            "[30]\ttest-logloss:0.01965\n",
            "[31]\ttest-logloss:0.01803\n",
            "[32]\ttest-logloss:0.01657\n",
            "[33]\ttest-logloss:0.01526\n",
            "[34]\ttest-logloss:0.01408\n",
            "[35]\ttest-logloss:0.01301\n",
            "[36]\ttest-logloss:0.01204\n",
            "[37]\ttest-logloss:0.01116\n",
            "[38]\ttest-logloss:0.01036\n",
            "[39]\ttest-logloss:0.00964\n",
            "[40]\ttest-logloss:0.00899\n",
            "[41]\ttest-logloss:0.00840\n",
            "[42]\ttest-logloss:0.00786\n",
            "[43]\ttest-logloss:0.00737\n",
            "[44]\ttest-logloss:0.00693\n",
            "[45]\ttest-logloss:0.00652\n",
            "[46]\ttest-logloss:0.00616\n",
            "[47]\ttest-logloss:0.00582\n",
            "[48]\ttest-logloss:0.00552\n",
            "[49]\ttest-logloss:0.00524\n",
            "[50]\ttest-logloss:0.00499\n",
            "[51]\ttest-logloss:0.00474\n",
            "[52]\ttest-logloss:0.00452\n",
            "[53]\ttest-logloss:0.00433\n",
            "[54]\ttest-logloss:0.00420\n",
            "[55]\ttest-logloss:0.00403\n",
            "[56]\ttest-logloss:0.00393\n",
            "[57]\ttest-logloss:0.00385\n",
            "[58]\ttest-logloss:0.00376\n",
            "[59]\ttest-logloss:0.00363\n",
            "[60]\ttest-logloss:0.00356\n",
            "[61]\ttest-logloss:0.00350\n",
            "[62]\ttest-logloss:0.00345\n",
            "[63]\ttest-logloss:0.00334\n",
            "[64]\ttest-logloss:0.00329\n",
            "[65]\ttest-logloss:0.00325\n",
            "[66]\ttest-logloss:0.00321\n",
            "[67]\ttest-logloss:0.00318\n",
            "[68]\ttest-logloss:0.00314\n",
            "[69]\ttest-logloss:0.00315\n",
            "[70]\ttest-logloss:0.00316\n",
            "[71]\ttest-logloss:0.00317\n",
            "[72]\ttest-logloss:0.00317\n",
            "[73]\ttest-logloss:0.00318\n",
            "[74]\ttest-logloss:0.00318\n",
            "[75]\ttest-logloss:0.00319\n",
            "[76]\ttest-logloss:0.00319\n",
            "[77]\ttest-logloss:0.00320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**xgb.DMatrix(...):** Converts data into an internal data structure (DMatrix) used by XGBoost.\n",
        "\n",
        "**train_pd['combinedFeatures'].tolist():** Converts the 'combinedFeatures' column from the pandas DataFrame train_pd to a list format.\n",
        "\n",
        "**label=train_pd['label']:** Specifies the labels for training (train_pd['label']).\n",
        "\n",
        "Similar preparation is done for the test data (test_pd).\n",
        "\n",
        "**params:** Dictionary containing parameters for configuring the XGBoost model.\n",
        "\n",
        "**objective='binary:logistic':** Specifies binary classification using logistic regression.\n",
        "\n",
        "**max_depth:** Maximum depth of a tree.\n",
        "\n",
        "eta: Learning rate.\n",
        "\n",
        "**eval_metric='logloss':** Metric used for evaluation during training.\n",
        "\n",
        "**xgb.train(...):** Trains an XGBoost model.\n",
        "\n",
        "**params:** Parameters for the model as defined earlier.\n",
        "\n",
        "**dtrain:** Training data (DMatrix object).\n",
        "\n",
        "num_boost_round=100: Number of boosting rounds (iterations).\n",
        "\n",
        "**evals=[(dtest, 'test')]:** Specifies evaluation data (dtest) and a name ('test') for it.\n",
        "\n",
        "**early_stopping_rounds=10:** Stops training if performance on evals does not improve for 10 rounds."
      ],
      "metadata": {
        "id": "LaRHOIbridIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "preds = bst.predict(dtest)\n",
        "predictions = [1 if x > 0.5 else 0 for x in preds]\n",
        "\n",
        "# Convert predictions and true labels to a Spark DataFrame\n",
        "pred_df = pd.DataFrame({'prediction': predictions, 'label': test_pd['label']})\n",
        "pred_spark_df = spark.createDataFrame(pred_df.itertuples(index=False), schema=['prediction', 'label'])\n",
        "print(pred_spark_df)\n",
        "print(pred_df)\n",
        "\n",
        "\n",
        "# Evaluate the accuracy\n",
        "correct_predictions = pred_spark_df.filter(pred_spark_df.prediction == pred_spark_df.label).count()\n",
        "total_data = pred_spark_df.count()\n",
        "accuracy = correct_predictions / total_data * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Lw-br-U4qm",
        "outputId": "d6ac0015-9739-46df-9cd7-53688bd7d167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[prediction: bigint, label: double]\n",
            "     prediction  label\n",
            "0             0    0.0\n",
            "1             0    0.0\n",
            "2             0    0.0\n",
            "3             1    1.0\n",
            "4             0    0.0\n",
            "..          ...    ...\n",
            "645           0    0.0\n",
            "646           0    0.0\n",
            "647           0    0.0\n",
            "648           0    0.0\n",
            "649           0    0.0\n",
            "\n",
            "[650 rows x 2 columns]\n",
            "Accuracy: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bst.predict(dtest):** Predicts probabilities using the trained XGBoost model bst on the test data (dtest).\n",
        "\n",
        "Converts the pandas DataFrame pred_df to a Spark DataFrame pred_spark_df using spark.createDataFrame().\n",
        "\n",
        "itertuples(index=False) iterates through rows of pred_df without including the index.\n",
        "\n",
        "Evaluates the accuracy by counting the number of correct predictions where 'prediction' matches 'label'.\n",
        "\n",
        "Calculates the total number of predictions (total_data).\n",
        "\n",
        "Computes the accuracy as a percentage (accuracy)."
      ],
      "metadata": {
        "id": "rWakSe9-jn3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'prediction' is the column you want to group by\n",
        "pred_spark_df.groupBy(\"prediction\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X4fGs3zwd9m",
        "outputId": "6ff38ce1-ec94-4797-9237-90b7cc2f50e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|  507|\n",
            "|         1|  143|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code pred_spark_df.groupBy(\"prediction\").count().show() groups the DataFrame pred_spark_df by the column named \"prediction\", counts the number of rows in each group, and displays the result. This is typically used to analyze the distribution of predictions or clusters generated by a machine learning model or algorithm."
      ],
      "metadata": {
        "id": "rxuIyZEMFIVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mathamatical Explanation of XGBoost**\n",
        "\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is an ensemble learning method that has gained popularity for its effectiveness in various machine learning tasks, particularly in structured/tabular data problems. Here's a brief overview of the mathematical background of XGBoost using LaTeX notation:\n",
        "\n",
        "Objective Function\n",
        "The objective of XGBoost is to minimize a regularized objective function, which can be generally expressed as:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝜙\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑙\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "∑\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "L(ϕ)=∑\n",
        "i=1\n",
        "n\n",
        "​\n",
        " l(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )+∑\n",
        "k=1\n",
        "K\n",
        "​\n",
        " Ω(f\n",
        "k\n",
        "​\n",
        " )\n",
        "\n",
        "where:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝜙\n",
        ")\n",
        "L(ϕ) is the objective function to be minimized.\n",
        "𝑛\n",
        "n is the number of training instances.\n",
        "𝑙\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "l(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " ) is the loss function that measures the difference between the true label\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  and the predicted label\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " .\n",
        "𝐾\n",
        "K is the number of trees (boosting rounds).\n",
        "𝑓\n",
        "𝑘\n",
        "f\n",
        "k\n",
        "​\n",
        "  represents the\n",
        "𝑘\n",
        "k-th tree in the ensemble.\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "Ω(f\n",
        "k\n",
        "​\n",
        " ) is the regularization term that penalizes complexity of the model.\n",
        "Loss Function and Regularization\n",
        "The loss function\n",
        "𝑙\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "l(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " ) typically depends on the specific problem (regression, classification, etc.) and can include:\n",
        "\n",
        "Regression:\n",
        "\n",
        "Squared Error:\n",
        "𝑙\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "l(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )=(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Binary Classification:\n",
        "\n",
        "Logistic Loss:\n",
        "𝑙\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "+\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        "⋅\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "l(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )=log(1+exp(−y\n",
        "i\n",
        "​\n",
        " ⋅\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " ))\n",
        "Multiclass Classification:\n",
        "\n",
        "Softmax Loss:\n",
        "𝑙\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "𝑦\n",
        "𝑖\n",
        "𝑗\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "𝑗\n",
        ")\n",
        "l(y\n",
        "i\n",
        "​\n",
        " ,\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )=−∑\n",
        "j\n",
        "​\n",
        " y\n",
        "ij\n",
        "​\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "ij\n",
        "​\n",
        " )\n",
        "Tree Ensemble\n",
        "XGBoost builds an ensemble of decision trees sequentially. Each tree\n",
        "𝑓\n",
        "𝑘\n",
        "f\n",
        "k\n",
        "​\n",
        "  is trained to correct the residuals of the previous tree and is added to the ensemble. The prediction\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        "  is computed as:\n",
        "\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        "=\n",
        "𝜙\n",
        "0\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "∑\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑓\n",
        "𝑘\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " =ϕ\n",
        "0\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " )+∑\n",
        "k=1\n",
        "K\n",
        "​\n",
        " f\n",
        "k\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " )\n",
        "\n",
        "where\n",
        "𝜙\n",
        "0\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "ϕ\n",
        "0\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ) is the initial prediction and\n",
        "𝑓\n",
        "𝑘\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "f\n",
        "k\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ) are the predictions from each tree in the ensemble.\n",
        "\n",
        "Regularization Terms\n",
        "The regularization term\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "Ω(f\n",
        "k\n",
        "​\n",
        " ) typically includes:\n",
        "\n",
        "Tree Complexity Regularization:\n",
        "\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        "𝑘\n",
        ")\n",
        "=\n",
        "𝛾\n",
        "𝑇\n",
        "+\n",
        "1\n",
        "2\n",
        "𝜆\n",
        "∥\n",
        "𝑤\n",
        "∥\n",
        "2\n",
        "2\n",
        "Ω(f\n",
        "k\n",
        "​\n",
        " )=γT+\n",
        "2\n",
        "1\n",
        "​\n",
        " λ∥w∥\n",
        "2\n",
        "2\n",
        "​\n",
        "\n",
        "𝑇\n",
        "T is the number of leaves in the tree.\n",
        "𝑤\n",
        "w are the weights associated with each leaf.\n",
        "Sparsity Regularization:\n",
        "\n",
        "Encourages sparse solutions."
      ],
      "metadata": {
        "id": "M9mh5whCFaYP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G69L9Ww1LgAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}